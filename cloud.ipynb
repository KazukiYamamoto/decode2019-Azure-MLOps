{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# クラウドへ（そしてその先へ）\n数値の問題を解決する方法については十分に検討しました。トレーニング部分をクラウドに移行します。（数値の問題ではこれ以上はローカルでの実行は不要ですが、他の問題については、ローカルでサブセットの問題をテストしてからクラウドに移動して全体を処理します）\n\nいくつか設定しましょう。\n\n最初にしなければならないことは、azureml.core パッケージがノートブック環境にインストールされているのを確認することです。Azure Notebooksを使用している場合は、簡単な2ステップのプロセスで確認できます。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Azure Notebooks に依存関係を追加する\n\"Project Settings\" をクリックします。\n\n![Project Setings](https://raw.githubusercontent.com/sethjuarez/pytorchintro/master/images/project_settings.png)\n\n次に、\"Environment\" タブでドロップダウンリストを左から順に `Requirements.txt` 、 `requirements.txt` 、 `Python 3.6` を選択します。\n\n![Settings](https://raw.githubusercontent.com/sethjuarez/pytorchintro/master/images/settings.png)\n\nこれらのステップで、実行できるようになるはずです。\n\n**注** もし上記の設定をしても問題が発生する場合は、Notebook でカーネルが Python 3.6 に設定されていることを確認してください。Python 3.6 になっていない場合は、次の操作で設定変更できます: ノートブックのメニューで [Kernel] -[Change Kernel] - [Python 3.6] を選択"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import json, os, sys\nimport time\nimport azureml\nfrom azureml.core.model import Model\nfrom azureml.core import Workspace, Run, Experiment\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.train.dnn import PyTorch\n#from azureml.widgets import RunDetails\n#from torchvision import datasets, transforms\n\nprint(\"Azure ML SDK Version: \", azureml.core.VERSION)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "No module named 'azureml.core'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e3a3ac80abd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWorkspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRunConfiguration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: No module named 'azureml.core'"
          ]
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Azure Machine Learning サービス（AzureML Service）のワークスペースを設定する"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## AzureML Service ワークスペース を作成する\n最初に必要な作業は、Azure Machine Learning サービス ワークスペースの作成です。\n以下の Python コードから AzureML Service のワークスペースを作成します。\n\n以下の Python スクリプトで\n- `ワークスペース名`\n- `リソースグループ名`\n- `AzureサブスクリプションID` \n- `Azureのリージョン`\n\nの4つの値を適宜設定して実行します。\n\nワークスペース名やリソースグループ名は、ご自身のAzure環境の中で区別できるものを任意で設定。\nAzureサブスクリプションIDには、Azureポータルで左メニューから「サブスクリプション」を選び、一覧表示されるサブスクリプションの中でワークスペースを作成する先のものを選んでコピー＆ペーストします。\nリージョンについては、AzureML Service が利用できるリージョンを設定します。\n\n既に AzureML Service ワークスペース を作成済みでそれを利用したい場合は、そのワークスペースの情報を設定しても構いません。\n\nスクリプトを実行すると `config.json` ファイルに設定が書き出されます。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "config = {}\nconfig[\"workspace_name\"] = \"decode2019_mlops\"\nconfig[\"resource_group\"] = \"decode2019\"\nconfig[\"subscription_id\"] = \"cd5e54ba-5b64-4acb-8ae5-72654d870add\"\nconfig[\"location\"] = \"southcentralus\"\n\nwith open('config.json', 'w') as f:\n    json.dump(config, f)",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "もし上記で設定した AzureML Service の ワークスペース に接続し、存在しない場合は新規に作成します。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.authentication import InteractiveLoginAuthentication\n\nprint(\"SDK Version:\", azureml.core.VERSION)\n#with open(\"aml_config/config.json\") as f:\n #   config = json.load(f)\n\nworkspace_name = config[\"workspace_name\"]\nresource_group = config[\"resource_group\"]\nsubscription_id = config[\"subscription_id\"]\nlocation = config[\"location\"]\n\ncli_auth = InteractiveLoginAuthentication()\n\ntry:\n    ws = Workspace.get(\n        name=workspace_name,\n        subscription_id=subscription_id,\n        resource_group=resource_group,\n        auth=cli_auth\n    )\n\nexcept:\n    # this call might take a minute or two.\n    print(\"Creating new workspace...\")\n    ws = Workspace.create(\n        name=workspace_name,\n        subscription_id=subscription_id,\n        resource_group=resource_group,\n        # create_resource_group=True,\n        location=location,\n        auth=cli_auth\n    )\n\n# print Workspace details\nprint(\"\\nWorkspace configuration succeeded. You are all set!\")\nprint(\"Using workspace below;\")\nprint(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep=\"\\n\")",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "SDK Version: 1.0.33\n",
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": "get_workspace error using subscription_id=cd5e54ba-5b64-4acb-8ae5-72654d870add, resource_group_name=decode2019, workspace_name=decode2019_mlops\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "Creating new workspace...\nDeploying KeyVault with name decode20keyvault16551d6c.\nDeploying StorageAccount with name decode20storageb922e2386.\nDeploying AppInsights with name decode20insights6ef874dc.\nDeployed AppInsights with name decode20insights6ef874dc.\nDeployed KeyVault with name decode20keyvault16551d6c.\nDeployed StorageAccount with name decode20storageb922e2386.\nDeploying Workspace with name decode2019_mlops.\nDeployed Workspace with name decode2019_mlops.\n\nWorkspace configuration succeeded. You are all set!\nUsing workspace below;\ndecode2019_mlops\ndecode2019\nsouthcentralus\ncd5e54ba-5b64-4acb-8ae5-72654d870add\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# クラウドコンピュート\n次に、実験用のコンピュートターゲットを定義する必要があります。これは新規のワークスペースなので、クラスタの名前は自由に変更してください（私は 'racer' と呼んでいます）。以下のコードは自分のクラスタへの参照を取得しようとしますが、存在しない場合は作成します。クラスタを作成する場合、少し時間がかかります。また、予想外の課金をされないように、実験が完了したらクラスターをオフにしてください（実際には、min_node を 0 に設定して、長時間アイドル状態になるとクラスタが自動的にオフになる設定を検討してください）。 \n\n**訳注** Azure の無償評価版などの GPU 最適化済みマシンを利用できない場合、またはコストを抑えたい場合は、vm_size を \"STANDARD_D2_V2\" にしてください。min_nodes を 1 以上にすると、訓練開始までの待ち時間を短縮できますが、コンピュートの削除し忘れなどで課金が継続されることがあるので注意してください。min_nodes を 0 にすると実行が終わると自動的にノードが削除されて課金されなくなります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "cluster = 'decode2019-MLOps'\ntry:\n    compute = ComputeTarget(workspace=ws, name=cluster)\n    print('Found existing compute target \"{}\"'.format(cluster))\nexcept ComputeTargetException:\n    print('Creating new compute target \"{}\"...'.format(cluster))\n    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6', min_nodes=0, max_nodes=6)\n    compute = ComputeTarget.create(ws, cluster, compute_config)\n    compute.wait_for_completion(show_output=True)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating new compute target \"decode2019-MLOps\"...\nCreating\nSucceeded\nAmlCompute wait for completion finished\nMinimum number of nodes requested have been provisioned\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 実験の時間\nコンピューティングターゲットが設定されたら、前回の小さなノートブックをリモートコンピューティング環境で実行できる単一のスクリプトにパッケージ化します。[あなたのために](train.py) train.py を作っておきました。実際、ファイルを見ると、前のノートブックから学んだものとまったく同じ概念がすべて表示されます（これはほとんどまったく同じですが、スクリプトへの受け渡しを容易にするために追加の事項を入れています）。\n\nAzure ML サービスには実験という概念があります。実験ごとに複数回実行することができます。ここでは、実験の実行方法を定義する Estimator オブジェクトを使用しています。\n\n### バックグラウンドで何をしてるか気にしないのであれば、ここは読む必要はありません\nバックグラウンドでは、Estimator は基本的に実験を格納する docker イメージの定義です。このすべてについての最もよい部分は、あなたがあなたの実験に使うもの（TensorFlowのカスタムバージョンであっても他の何かであっても）に関係なく、それが必ず実行可能であるということです - 結局それはコンテナです。とても使いやすいです。\n\n### 通常の手順に戻る\nEstimator を Azure ML サービスで実行することを送信すると、現在のディレクトリの内容がコピーされ、新しいコンテナにまとめられます（それらは [.amlignore] ファイルに記述されたもの以外、全部アップロードされます）\n\nまた、'argparse' を使用しているので、推論器の定義の一部としてトレーニングスクリプトに外部パラメータを指定できます。\n\n次の3行を実行して、何が起こるのか見てみましょう。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 実験を作成\nmnist = Experiment(ws, 'pytorchmnist')\n\n# script parameters\nscript_params={\n    '--epochs': 5,\n    '--batch': 100,\n    '--lr': .001,\n    '--model': 'cnn'\n}\n\n# Estimator を作成\nestimator = PyTorch(source_directory='.',\n                       compute_target=compute, \n                       entry_script='train.py',\n                       script_params=script_params,\n                       use_gpu=True)\n\nrun = mnist.submit(estimator)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Submitting /home/nbuser/library/decode2019-Azure-MLOps directory for run. The size of the directory >= 25 MB, so it can take a few minutes.\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/html": "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>pytorchmnist</td><td>pytorchmnist_1557485074_a33bf082</td><td>azureml.scriptrun</td><td>Queued</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/cd5e54ba-5b64-4acb-8ae5-72654d870add/resourceGroups/decode2019/providers/Microsoft.MachineLearningServices/workspaces/decode2019_mlops/experiments/pytorchmnist/runs/pytorchmnist_1557485074_a33bf082\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>",
            "text/plain": "Run(Experiment: pytorchmnist,\nId: pytorchmnist_1557485074_a33bf082,\nType: azureml.scriptrun,\nStatus: Queued)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Status が　`Queued`　になっていれば、キューにジョブが追加され、実行待ちの状態です。\n以下の処理を起動すると、ジョブの現在の状態が表示されます。\n\n表示は自動で更新されますので、ジョブの実行が開始され（Running）、完了（Completed）するまで数分ほど待ちましょう。"
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true
      },
      "cell_type": "code",
      "source": "#RunDetails(run).show()\nrun.wait_for_completion()",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6850fb792e144fb39e231b969b24e10a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', '…"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "すべて完了すると、次のようになります:\n\n![AzureML Run](https://raw.githubusercontent.com/sethjuarez/pytorchintro/master/images/run_widget.png)\n\n実際に、損失関数は時間の経過とともに（平均して）減少し、モデルの精度が上がることに注意してください。learning_rate パラメータを変更して試してみてください。詳しくは、[Azure Machine Learning service でモデルのハイパーパラメーターを調整する](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-tune-hyperparameters) を参照してください。\n\nさて、どのようにしてこれらの素晴らしいチャートが表示できたのか疑問に思うかもしれません。これは Azure ML サービスが、実験結果に対して実用的な価値を付加してくれるところです。[いくつか](https://github.com/sethjuarez/pytorchintro/blob/master/train.py#L156-L166) の [戦略的](https://github.com/sethjuarez/pytorchintro/blob/master/train.py#L121-L122) に [配置](https://github.com/sethjuarez/pytorchintro/blob/master/train.py#L142-L143) されたログステートメントを使用して、Azure ML サービスはこの出力を作成しました。実際、値が複数回ログに記録されると、テーブル内の項目ではなくチャートが自動的に作成されます。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# モデル\nトレーニングがすべて完了して出力が完了したら、実際に特定の実験のすべての実行の出力を確認し、それを「公式な」ワークスペースモデルに昇格させることができます。重要なファイル（つまり私たちをお金持ちにしてくれるかもしれないモデル）が通常 Jeff という名前のコンピュータ上に置かれるのは素晴らしい機能です。現在は、多くの人がモデルのバージョン管理さえしていませんが、以下のコードを実行してください。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "run.get_file_names()",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "['azureml-logs/55_batchai_execution.txt',\n 'azureml-logs/60_control_log.txt',\n 'azureml-logs/80_driver_log.txt',\n 'azureml-logs/azureml.log',\n 'outputs/model.onnx',\n 'outputs/model.pth']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_file = 'outputs/model.pth'\nrun.download_file(name=model_file, output_file_path='model.pth')\nmodel = Model.register(ws, model_name='PyTorchMNIST', model_path='model.pth', \n                       description='CNN PyTorch Model')",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Registering model PyTorchMNIST\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# イメージ\nモデルが完成したので、それをプロダクションで使用する場合は、モデルの使用方法を定義する必要があります。これはスコアリングまたは推論とも呼ばれます。Azure ML サービスでは、基本的に2つのメソッドが必要です:\n1. `init()`\n2. `run(raw)` - JSON 文字列を取り込んで予測を返す\n\n最初にスコアリングスクリプトが実行される環境を記述し、それを設定ファイルにまとめる必要があります。"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "myenv = CondaDependencies()\nmyenv.add_pip_package('numpy')\nmyenv.add_pip_package('torch')\nwith open('pytorchmnist.yml','w') as f:\n    print('Writing out {}'.format('pytorchmnist.yml'))\n    f.write(myenv.serialize_to_string())\n    print('Done!')",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Writing out pytorchmnist.yml\nDone!\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "次に、Azure ML サービスにスコアリングスクリプトの場所を通知する必要があります。score.py を [あらかじめ作っておきました](score.py)。ファイルを見ると、init() メソッドと run(raw) メソッドの両方が簡単に見つかるはずです。ファイルをローカルで実行して、正しい動作をしていることを確認することもできます。\n\nこれですべてが完成したので、イメージを作成しましょう。\n\n### バックグラウンドで何をしてるか気にしないのであれば、ここは読む必要はありません\n基本的には、定義からdockerイメージを作成して、Workspace に表示される Azure Container Registry にプッシュします。"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "**注** しばらく時間がかかります"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from azureml.core.image import ContainerImage, Image\n\n# イメージの作成\nimage_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                runtime=\"python\", \n                                conda_file=\"pytorchmnist.yml\")\n\nimage = Image.create(ws, 'pytorchmnist', [model], image_config)\nimage.wait_for_creation(show_output=True)",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Creating image\nRunning.................................................\nSucceededImage creation operation finished for image pytorchmnist:1, operation \"Succeeded\"\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# デプロイ\nイメージ作成をせずに、残りの展開プロセスを Azure Pipelines のようなものに移動したいかもしれません。そうではなくて、このサービスを引き続きワークスペースにデプロイしたい場合は、以下を使用してください。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "from azureml.core.webservice import Webservice, AciWebservice\n\nservice_name = 'pytorchmnist-svc'\n\n# check for existing service\nsvcs = [svc for svc in Webservice.list(ws) if svc.name==service_name]\nif len(svcs) == 1:\n    print('Deleting prior {} deployment'.format(service_name))\n    svcs[0].delete()\n\n# create service\naciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                                            memory_gb=1, \n                                            description='simple MNIST digit detection')\nservice = Webservice.deploy_from_image(workspace=ws, \n                                    image=image, \n                                    name=service_name, \n                                    deployment_config=aciconfig)\nservice.wait_for_deployment(show_output=True)\nprint(service.scoring_uri)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "イメージを ACI またはワークスペース Kubernetes クラスターにプッシュすることもできます。\n\n時々うまくいかないことがあります・・・もし実行時にそうなったら、実際の [logs](deploy.log) を見てください。!"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "with open('deploy.log','w') as f:\n    f.write(service.get_logs())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# サービスの実行\n以上でサービスは動作しています。適切に動作しているか見てみましょう。前から使っているテストデータをロードしてランダムな数字で試すことができます。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#digits = datasets.MNIST('data', train=True, download=True,\n#                        transform=transforms.Compose([\n#                            transforms.ToTensor(),\n#                            transforms.Lambda(lambda x: x.reshape(28*28))\n#                        ]),\n#                        target_transform=transforms.Compose([\n#                            transforms.Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, y, 1))\n#                        ])\n#                     )\n#print(len(digits))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "インデックスとして基本的に最大60,000まで任意の数を選ぶことができます。サービスがどのように動作しているかを見るために何回か試してみてください。"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#import torch\n#from PIL import Image\n#import matplotlib.pyplot as plt\n\n#X, Y = digits[57435]\n#X = X * 255\n#plt.imshow(255 - X.reshape(28,28), cmap='gray')\n#print(Y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# ポストしようとしているエンドポイントの場所\n#image_str = ','.join(map(str, X.int().tolist()))\n#print(image_str)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "#import json\n#import requests\n#service_url = service.scoring_uri\n#print(service_url)\n#r = requests.post(service_url, json={'image': image_str })\n#r.json()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## 最後に\nこの小さな旅が参考になっていればうれしいです！ 私の目標は、機械学習の基本がそれほど悪いものではないと理解してもらうことです。コメント、提案、または分からないところは一言教えてください。"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}